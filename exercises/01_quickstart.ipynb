{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This notebook is just to make sure that everything works. Firstly, let's pull the latest changes for this repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's install the requirements\n",
    "!pip3 install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Option 2 - Using Access Key\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = os.environ.get(\"OPENAI_API_BASE\")\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai.api_version = os.environ.get('OPENAI_API_VERSION', \"2022-12-01\")\n",
    "\n",
    "# Define embedding model and encoding\n",
    "EMBEDDING_MODEL = os.environ.get('OPENAI_EMBEDDING_MODE', 'text-embedding-ada-002')\n",
    "COMPLETION_MODEL = os.environ.get('OPENAI_COMPLETION_MODEL', 'gpt-35-turbo')\n",
    "encoding = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if tokenizer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_encode = \"Hello world!\"\n",
    "tokens = encoding.encode(text_to_encode)\n",
    "print(f\"There are {len(tokens)} for text prompt: '{text_to_encode}'\")\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if we can reach OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Answer shortly \"Who is the prime minister of Israel?\"'  # The prompt to generate completions for\n",
    "# Generate 3 completions\n",
    "response = openai.Completion.create(engine=\"gpt-35-turbo\",\n",
    "                                    prompt=prompt,\n",
    "                                    temperature=0,\n",
    "                                    max_tokens=10,\n",
    "                                    top_p=1\n",
    "                                    )  # Change the temperature to generate more or less random completions\n",
    "answer = response.choices[0].text\n",
    "print(f\"{answer}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it in a streaming fashion from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resp in openai.Completion.create(\n",
    "    engine='gpt-35-turbo', \n",
    "    prompt='Give me 5 taglines for an ice cream shop', \n",
    "    max_tokens=512, \n",
    "    stream=True,\n",
    "    temperature=0.7):\n",
    "    sys.stdout.write(resp.choices[0].text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save costs we would like to use a local embedding code instead of working with API. Lets test if Local embedding model work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = ['Hello World!']\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(f\"Got embeddings with shape {embeddings.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the embedding length coming from OpenAi's remote model (text-embedding-ada-002):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use it only once and then the local embeddings.\n",
    "response = openai.Embedding.create(input=\"Hello World!\", engine=EMBEDDING_MODEL)\n",
    "e = response[\"data\"][0][\"embedding\"]\n",
    "print(\"Embedding shape\", len(e))\n",
    "\n",
    "\n",
    "print('usage', response['usage'])\n",
    "print('model', response['model'])\n",
    "print(\"Full response keys from embedding\", response.keys())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai-qna-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ee1bbf3137c7ea9420c4fd488a55642063e5739fe2a7286130d9ba47405b69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
